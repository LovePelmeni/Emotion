# Multimodal Machine Learning for refined Emotion Recognition

Author: Klimushin Kirill Alexandrovich

<img>

## Motivation
Modern society been going through significant changes, which impact Educational systems, forcing them to search for alternative ways of providing high-quality studying experience, such as conducting and organizing remote sessions outdoors or online via popular and ubiqitous video streaming platforms. However, with this considerable changes, pertinent concern of effectiveness may arise.

## Introduction

This document is regarded as an comprehensive guide to Multimodal network design, focused on refining and improving currently available Emotion Recognition solutions in multi-party conversations / dialogs on videos.

## Scope
Document is responsible for informing reader of relevant architectural decisions, design concerns and usability sections, where . All corresponding techinical resources are provided under 'References' section.

## Architecture

<p align="center">
  <a><img src="https://github.com/LovePelmeni/EmotionVIT/blob/main/docs/imgs/architecture.png" style="width: 100%; height: 100%"></a>
</p>

## Image Modality
<img>

## Audio Modality
<img>

## Text Modality
<img>

## Data Integration Strategy
<explaining which fusion strategy has been leveraged and why>

## Evaluation and Validation
<explain how model will be validated>

## Interpretability
<explain how network will be validated>

## Security and Privacy
<explain potential security concerns, that may arise>
## References

- [Towards Visualizing Multimodal networks by Paul Pu Liang, Yiwei Lyu, Gunjan Chhablani]("https://arxiv.org/pdf/2207.00056.pdf")